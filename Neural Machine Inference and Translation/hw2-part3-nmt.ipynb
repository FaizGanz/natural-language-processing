{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5mRDji1iNsc"
   },
   "source": [
    "# Fall 2022: DS-GA 1011 NLP with Representation Learning\n",
    "## Homework 2\n",
    "## Part 3: Neural Machine Translation (30 pts)\n",
    "In this part, you implement Transformer encoder for Neural Machine Translation (NMT) using a sequence to sequence (seq2seq) model for English to French translation with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki8Rdu4IiNsd"
   },
   "source": [
    "---\n",
    "### 1 Transformer Encoder (18 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OB4991PPiNse"
   },
   "outputs": [],
   "source": [
    "# Add utilities path\n",
    "import sys\n",
    "\n",
    "path_to_utils = 'pyfiles'\n",
    "sys.path.append(path_to_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1h61HxKEiNsi"
   },
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "import global_variables\n",
    "import nmt_dataset\n",
    "import nnet_models_new\n",
    "import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "juHsWAmriNsl"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "import os\n",
    "\n",
    "source_name = 'en'\n",
    "target_name = 'fr'\n",
    "\n",
    "base_saved_models_dir = '.'\n",
    "saved_models_dir = os.path.join(base_saved_models_dir, source_name+'2'+target_name)\n",
    "\n",
    "main_data_path = './data/'\n",
    "\n",
    "path_to_train_data = {'source':main_data_path+'train.'+source_name, \n",
    "                      'target':main_data_path+'train.'+target_name}\n",
    "path_to_val_data = {'source': main_data_path+'valid.'+source_name, \n",
    "                      'target':main_data_path+'valid.'+target_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "4ZISJFayiNso"
   },
   "outputs": [],
   "source": [
    "saved_language_model_dir = os.path.join(saved_models_dir, 'lang_obj')\n",
    "\n",
    "dataset_dict = {'train': nmt_dataset.LanguagePair(source_name = source_name, target_name=target_name, \n",
    "                    filepath = path_to_train_data, \n",
    "                    lang_obj_path = saved_language_model_dir,\n",
    "                     minimum_count = 1), \n",
    "\n",
    "                'val': nmt_dataset.LanguagePair(source_name = source_name, target_name=target_name, \n",
    "                    filepath = path_to_val_data, \n",
    "                    lang_obj_path = saved_language_model_dir,\n",
    "                    minimum_count = 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "9F04tskaiNsr"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = int(dataset_dict['train'].main_df['source_len'].quantile(0.9999))\n",
    "batchSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "-5GP1oyqiNsv"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_dict = {'train': DataLoader(dataset_dict['train'], batch_size = batchSize, \n",
    "                            collate_fn = partial(nmt_dataset.vocab_collate_func, MAX_LEN=MAX_LEN),\n",
    "                            shuffle = True, num_workers=0), \n",
    "                    'val': DataLoader(dataset_dict['val'], batch_size = batchSize, \n",
    "                            collate_fn = partial(nmt_dataset.vocab_collate_func, MAX_LEN=MAX_LEN),\n",
    "                            shuffle = True, num_workers=0) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "l_T7Fi87iNsy"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "source_lang_obj = dataset_dict['train'].source_lang_obj\n",
    "target_lang_obj = dataset_dict['train'].target_lang_obj\n",
    "\n",
    "source_vocab = dataset_dict['train'].source_lang_obj.n_words;\n",
    "target_vocab = dataset_dict['train'].target_lang_obj.n_words;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bghnW3YiNs2"
   },
   "source": [
    "#### 1.1 Encoder (9 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "O4T23W8liNs4"
   },
   "outputs": [],
   "source": [
    "# Add transformer as encoder in seq2seq model\n",
    "\n",
    "# code below can help you to start it, but feel free to start from scratch\n",
    "\n",
    "class EncoderTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.d_hid = d_hid\n",
    "        self.nlayers = nlayers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.encoder = nn.Embedding(ntoken, d_model, padding_idx = global_variables.PAD_IDX)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, nlayers)\n",
    "        \n",
    "    def forward(self, text_vec):\n",
    "        src = self.encoder(text_vec) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "#         print(output.shape, torch.zeros_like(output.mean(dim=1).unsqueeze(dim=0)).shape)\n",
    "        return output, torch.zeros_like(output.mean(dim=1).unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = source_vocab  # size of vocabulary\n",
    "emsize = 512  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_tf = EncoderTransformer(ntokens, emsize, nhead, d_hid, nlayers, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcUYkz_RiNs9"
   },
   "source": [
    "#### 1.2 Decoder(s) (9 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layers = 1\n",
    "hidden_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "OT_JZXWeiNs9"
   },
   "outputs": [],
   "source": [
    "# Basic RNN decoder (no attention)\n",
    "decoder_rnn_basic = nnet_models_new.DecoderRNN(target_vocab, hidden_size, rnn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "U6DmOcLViNtB"
   },
   "outputs": [],
   "source": [
    "# RNN Decoder with Encoder attention\n",
    "decoder_enc_attn = nnet_models_new.Decoder_SelfAttn(target_vocab, hidden_size, encoder_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "szrb9vkiiNtE"
   },
   "outputs": [],
   "source": [
    "# RNN Decoder with Encoder & Self attention\n",
    "decoder_enc_selfattn = nnet_models_new.Decoder_SelfAttn(target_vocab, hidden_size, self_attention=True, encoder_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01;\n",
    "longest_label = 1;\n",
    "gradient_clip = 0.3;\n",
    "use_cuda = True\n",
    "num_epochs = 20\n",
    "\n",
    "nmt_rnn = nnet_models_new.seq2seq(encoder_tf, decoder_rnn_basic,\n",
    "                              lr = lr, \n",
    "                              use_cuda = use_cuda, \n",
    "                              hiddensize = hidden_size, \n",
    "                              numlayers = hidden_size, # variable no used anywhere in the class\n",
    "                              target_lang=dataset_dict['train'].target_lang_obj,\n",
    "                              longest_label = longest_label,\n",
    "                              clip = gradient_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_dYn8C_iNtH"
   },
   "source": [
    "#### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import notebook\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_filepath(path, enc_type):\n",
    "    filename = 'nmt_enc_'+enc_type+'_dec_rnn.pth'\n",
    "    return os.path.join(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(nmt_model, path, enc_type):\n",
    "    if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    filename = 'nmt_enc_'+enc_type+'_dec_rnn.pth'\n",
    "    torch.save(nmt_model, os.path.join(path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "N1-h6PlgiNtI"
   },
   "outputs": [],
   "source": [
    "def train_model(dataloader, nmt, num_epochs=50, val_every=1, saved_model_path = '.', enc_type ='rnn'):\n",
    "\n",
    "    best_bleu = -1;\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        start = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        print('Epoch: [{}/{}]'.format(epoch, num_epochs));\n",
    "        \n",
    "        for i, data in notebook.tqdm(enumerate(dataloader['train']), total=len(dataloader['train'])):  \n",
    "            _, curr_loss = nmt.train_step(data);\n",
    "            running_loss += curr_loss\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader['train']) \n",
    "        \n",
    "        print(\"epoch {} loss = {}, time = {}\".format(epoch, epoch_loss,\n",
    "                                                        time.time() - start))\n",
    "        sys.stdout.flush()\n",
    "   \n",
    "        if epoch%val_every == 0:\n",
    "            val_bleu_score = nmt.get_bleu_score(dataloader['val']);\n",
    "            print('validation bleu: ', val_bleu_score)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            nmt.scheduler_step(val_bleu_score);\n",
    "            \n",
    "            if val_bleu_score > best_bleu:\n",
    "                best_bleu = val_bleu_score\n",
    "                save_models(nmt, saved_model_path, enc_type);\n",
    "\n",
    "        print('='*50)\n",
    "\n",
    "    print(\"Training completed. Best BLEU is {}\".format(best_bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201e379097404ed9aba1c7c53e8d8878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [119]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     nmt_rnn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(get_full_filepath(saved_models_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrnn\u001b[39m\u001b[38;5;124m'\u001b[39m), map_location\u001b[38;5;241m=\u001b[39mglobal_variables\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnmt_rnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                      \u001b[49m\u001b[43msaved_model_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msaved_models_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                      \u001b[49m\u001b[43menc_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrnn_test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [118]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(dataloader, nmt, num_epochs, val_every, saved_model_path, enc_type)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, num_epochs));\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m notebook\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])):  \n\u001b[1;32m---> 12\u001b[0m     _, curr_loss \u001b[38;5;241m=\u001b[39m \u001b[43mnmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[0;32m     13\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m curr_loss\n\u001b[0;32m     15\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\hw2\\hw2\\pyfiles\\nnet_models_new.py:417\u001b[0m, in \u001b[0;36mseq2seq.train_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    410\u001b[0m decoder_output, decoder_hidden, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(decoder_input,\n\u001b[0;32m    411\u001b[0m                                               encoder_hidden, \n\u001b[0;32m    412\u001b[0m                                               encoder_output,\n\u001b[0;32m    413\u001b[0m                                               xs_len)\n\u001b[0;32m    416\u001b[0m scores \u001b[38;5;241m=\u001b[39m decoder_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, decoder_output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 417\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_params()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cds-nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cds-nlp\\lib\\site-packages\\torch\\nn\\modules\\loss.py:216\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cds-nlp\\lib\\site-packages\\torch\\nn\\functional.py:2701\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2700\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'"
     ]
    }
   ],
   "source": [
    "train_again = False\n",
    "if os.path.exists(get_full_filepath(saved_models_dir, 'rnn')) and (not train_again):\n",
    "    nmt_rnn = torch.load(get_full_filepath(saved_models_dir, 'rnn'), map_location=global_variables.device)\n",
    "else:\n",
    "    train_model(dataloader_dict, nmt_rnn, \n",
    "                      num_epochs = num_epochs, \n",
    "                      saved_model_path = saved_models_dir, \n",
    "                      enc_type = 'rnn_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLef3aD5iNtM"
   },
   "source": [
    "---\n",
    "### 2 Attention visualization (12 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVcysNCtiNtN"
   },
   "outputs": [],
   "source": [
    "# Model was trained in ~2 hours, i.e. you can expect attention maps\n",
    "# to look quite 'hard' (less soft spreading) i.e. attending to some particular token in the input"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "hw2-part3-nmt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
